% This file was created with JabRef 2.10.
% Encoding: UTF-8

@article{fourier_feature_networks,
  year      = {2020},
  title     = {{Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains}},
  author    = {Tancik, Matthew and Srinivasan, Pratul P and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T and Ng, Ren},
  journal   = {arXiv},
  eprint    = {2006.10739},
  abstract  = {{We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.}},
  keywords  = {},
  local-url = {file://localhost/Users/Ryan/Documents/Papers%20Library/Tancik-Fourier%20Features%20Let%20Networks%20Learn%20High%20Frequency%20Functions%20in%20Low%20Dimensional%20Domains-2020-arXiv.pdf}
}

@article{surgical_video_translation,
  year      = {2021},
  rating    = {5},
  title     = {{Long-Term Temporally Consistent Unpaired Video Translation from Simulated Surgical 3D Data}},
  author    = {Rivoir, Dominik and Pfeiffer, Micha and Docea, Reuben and Kolbinger, Fiona and Riediger, Carina and Weitz, Jürgen and Speidel, Stefanie},
  journal   = {arXiv},
  eprint    = {2103.17204},
  abstract  = {{Research in unpaired video translation has mainly focused on short-term temporal consistency by conditioning on neighboring frames. However for transfer from simulated to photorealistic sequences, available information on the underlying geometry offers potential for achieving global consistency across views. We propose a novel approach which combines unpaired image translation with neural rendering to transfer simulated to photorealistic surgical abdominal scenes. By introducing global learnable textures and a lighting-invariant view-consistency loss, our method produces consistent translations of arbitrary views and thus enables long-term consistent video synthesis. We design and test our model to generate video sequences from minimally-invasive surgical abdominal scenes. Because labeled data is often limited in this domain, photorealistic data where ground truth information from the simulated domain is preserved is especially relevant. By extending existing image-based methods to view-consistent videos, we aim to impact the applicability of simulated training and evaluation environments for surgical applications. Code and data: http://opencas.dkfz.de/video-sim2real.}},
  keywords  = {},
  local-url = {file://localhost/Users/Ryan/Documents/Papers%20Library/Rivoir-Long-Term%20Temporally%20Consistent%20Unpaired%20Video%20Translation%20from%20Simulated%20Surgical%203D%20Data-2021-arXiv.pdf}
}

@article{munit, 
year = {2018}, 
title = {{Multimodal Unsupervised Image-to-Image Translation}}, 
author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan}, 
journal = {arXiv}, 
eprint = {1804.04732}, 
abstract = {{Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT}}, 
keywords = {}, 
local-url = {file://localhost/Users/Ryan/Documents/Papers%20Library/Huang-Multimodal%20Unsupervised%20Image-to-Image%20Translation-2018-arXiv.pdf}
}

@article{surgical_image_translation,
  year      = {2019},
  title     = {{Generating large labeled data sets for laparoscopic image processing tasks using unpaired image-to-image translation}},
  author    = {Pfeiffer, Micha and Funke, Isabel and Robu, Maria R and Bodenstedt, Sebastian and Strenger, Leon and Engelhardt, Sandy and Roß, Tobias and Clarkson, Matthew J and Gurusamy, Kurinchi and Davidson, Brian R and Maier-Hein, Lena and Riediger, Carina and Welsch, Thilo and Weitz, Jürgen and Speidel, Stefanie},
  journal   = {arXiv},
  eprint    = {1907.02882},
  abstract  = {{In the medical domain, the lack of large training data sets and benchmarks is often a limiting factor for training deep neural networks. In contrast to expensive manual labeling, computer simulations can generate large and fully labeled data sets with a minimum of manual effort. However, models that are trained on simulated data usually do not translate well to real scenarios. To bridge the domain gap between simulated and real laparoscopic images, we exploit recent advances in unpaired image-to-image translation. We extent an image-to-image translation method to generate a diverse multitude of realistically looking synthetic images based on images from a simple laparoscopy simulation. By incorporating means to ensure that the image content is preserved during the translation process, we ensure that the labels given for the simulated images remain valid for their realistically looking translations. This way, we are able to generate a large, fully labeled synthetic data set of laparoscopic images with realistic appearance. We show that this data set can be used to train models for the task of liver segmentation of laparoscopic images. We achieve average dice scores of up to 0.89 in some patients without manually labeling a single laparoscopic image and show that using our synthetic data to pre-train models can greatly improve their performance. The synthetic data set will be made publicly available, fully labeled with segmentation maps, depth maps, normal maps, and positions of tools and camera (http://opencas.dkfz.de/image2image).}},
  keywords  = {},
}