% This file was created with JabRef 2.10.
% Encoding: UTF-8
@String(PAMI = {IEEE Transactions on Pattern Analysis and Machine Intelligence})
@String(CVPR= {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)})
@String(ICCV= {Proceedings of the International Conference on Computer Vision (ICCV)})
@String(ECCV= {Proceedings of the European Conference on Computer Vision (ECCV)})
@String(NIPS= {Advances in Neural Information Processing Systems (NeurIPS)})
@string(ICLR = {Proceedings of the International Conference on Learning Representations (ICLR)})
@String(PR   = {Pattern Recognition})
@String(AAAI = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)})
@String(ICML = {Proceedings of the International Conference on Machine Learning (ICML)})
@String(CORL = {Conference on Robot Learning (CoRL)})
@String(BMVC = {The British Machine Vision Conference (BMVC)})
@String(WACV = {IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)})
@String(ICRA = {IEEE International Conference on Robotics and Automation (ICRA)})
@String(IROS = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)})
@String(RAL = {IEEE Robotics and Automation Letters (RA-L)})


@article{fourier_feature_networks,
  year     = {2020},
  title    = {{Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains}},
  author   = {Tancik, Matthew and Srinivasan, Pratul P and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T and Ng, Ren},
  journal  = {arXiv},
  eprint   = {2006.10739},
  abstract = {{We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.}}
}

@article{surgical_video_translation,
  year     = {2021},
  rating   = {5},
  title    = {{Long-Term Temporally Consistent Unpaired Video Translation from Simulated Surgical 3D Data}},
  author   = {Rivoir, Dominik and Pfeiffer, Micha and Docea, Reuben and Kolbinger, Fiona and Riediger, Carina and Weitz, Jürgen and Speidel, Stefanie},
  journal  = {arXiv},
  eprint   = {2103.17204},
  abstract = {{Research in unpaired video translation has mainly focused on short-term temporal consistency by conditioning on neighboring frames. However for transfer from simulated to photorealistic sequences, available information on the underlying geometry offers potential for achieving global consistency across views. We propose a novel approach which combines unpaired image translation with neural rendering to transfer simulated to photorealistic surgical abdominal scenes. By introducing global learnable textures and a lighting-invariant view-consistency loss, our method produces consistent translations of arbitrary views and thus enables long-term consistent video synthesis. We design and test our model to generate video sequences from minimally-invasive surgical abdominal scenes. Because labeled data is often limited in this domain, photorealistic data where ground truth information from the simulated domain is preserved is especially relevant. By extending existing image-based methods to view-consistent videos, we aim to impact the applicability of simulated training and evaluation environments for surgical applications. Code and data: http://opencas.dkfz.de/video-sim2real.}}
}

@article{munit,
  year     = {2018},
  title    = {{Multimodal Unsupervised Image-to-Image Translation}},
  author   = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
  journal  = {arXiv},
  eprint   = {1804.04732},
  abstract = {{Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT}}
}

@article{surgical_image_translation,
  year     = {2019},
  title    = {{Generating large labeled data sets for laparoscopic image processing tasks using unpaired image-to-image translation}},
  author   = {Pfeiffer, Micha and Funke, Isabel and Robu, Maria R and Bodenstedt, Sebastian and Strenger, Leon and Engelhardt, Sandy and Roß, Tobias and Clarkson, Matthew J and Gurusamy, Kurinchi and Davidson, Brian R and Maier-Hein, Lena and Riediger, Carina and Welsch, Thilo and Weitz, Jürgen and Speidel, Stefanie},
  journal  = {arXiv},
  eprint   = {1907.02882},
  abstract = {{In the medical domain, the lack of large training data sets and benchmarks is often a limiting factor for training deep neural networks. In contrast to expensive manual labeling, computer simulations can generate large and fully labeled data sets with a minimum of manual effort. However, models that are trained on simulated data usually do not translate well to real scenarios. To bridge the domain gap between simulated and real laparoscopic images, we exploit recent advances in unpaired image-to-image translation. We extent an image-to-image translation method to generate a diverse multitude of realistically looking synthetic images based on images from a simple laparoscopy simulation. By incorporating means to ensure that the image content is preserved during the translation process, we ensure that the labels given for the simulated images remain valid for their realistically looking translations. This way, we are able to generate a large, fully labeled synthetic data set of laparoscopic images with realistic appearance. We show that this data set can be used to train models for the task of liver segmentation of laparoscopic images. We achieve average dice scores of up to 0.89 in some patients without manually labeling a single laparoscopic image and show that using our synthetic data to pre-train models can greatly improve their performance. The synthetic data set will be made publicly available, fully labeled with segmentation maps, depth maps, normal maps, and positions of tools and camera (http://opencas.dkfz.de/image2image).}}
}

@article{msssim,
  year     = {2015},
  title    = {{Learning to Generate Images with Perceptual Similarity Metrics}},
  author   = {Snell, Jake and Ridgeway, Karl and Liao, Renjie and Roads, Brett D and Mozer, Michael C and Zemel, Richard S},
  journal  = {arXiv},
  eprint   = {1511.06409},
  abstract = {{Deep networks are increasingly being applied to problems involving image synthesis, e.g., generating images from textual descriptions and reconstructing an input image from a compact representation. Supervised training of image-synthesis networks typically uses a pixel-wise loss (PL) to indicate the mismatch between a generated image and its corresponding target image. We propose instead to use a loss function that is better calibrated to human perceptual judgments of image quality: the multiscale structural-similarity score (MS-SSIM). Because MS-SSIM is differentiable, it is easily incorporated into gradient-descent learning. We compare the consequences of using MS-SSIM versus PL loss on training deterministic and stochastic autoencoders. For three different architectures, we collected human judgments of the quality of image reconstructions. Observers reliably prefer images synthesized by MS-SSIM-optimized models over those synthesized by PL-optimized models, for two distinct PL measures (\$\textbackslashell\_1\$ and \$\textbackslashell\_2\$ distances). We also explore the effect of training objective on image encoding and analyze conditions under which perceptually-optimized representations yield better performance on image classification. Finally, we demonstrate the superiority of perceptually-optimized networks for super-resolution imaging. Just as computer vision has advanced through the use of convolutional architectures that mimic the structure of the mammalian visual system, we argue that significant additional advances can be made in modeling images through the use of training objectives that are well aligned to characteristics of human perception.}},
  keywords = {}
}

@article{lsgan,
  year      = {2016},
  title     = {{Least Squares Generative Adversarial Networks}},
  author    = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y K and Wang, Zhen and Smolley, Stephen Paul},
  journal   = {arXiv},
  eprint    = {1611.04076},
  abstract  = {{Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson \$\textbackslashchi\textasciicircum2\$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.}},
  keywords  = {},
}


@article{retina_gan,
  year     = {2020},
  title    = {{RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer}},
  author   = {Ho, Daniel and Rao, Kanishka and Xu, Zhuo and Jang, Eric and Khansari, Mohi and Bai, Yunfei},
  journal  = {arXiv},
  eprint   = {2011.03148},
  abstract = {{The success of deep reinforcement learning (RL) and imitation learning (IL) in vision-based robotic manipulation typically hinges on the expense of large scale data collection. With simulation, data to train a policy can be collected efficiently at scale, but the visual gap between sim and real makes deployment in the real world difficult. We introduce RetinaGAN, a generative adversarial network (GAN) approach to adapt simulated images to realistic ones with object-detection consistency. RetinaGAN is trained in an unsupervised manner without task loss dependencies, and preserves general object structure and texture in adapted images. We evaluate our method on three real world tasks: grasping, pushing, and door opening. RetinaGAN improves upon the performance of prior sim-to-real methods for RL-based object instance grasping and continues to be effective even in the limited data regime. When applied to a pushing task in a similar visual domain, RetinaGAN demonstrates transfer with no additional real data requirements. We also show our method bridges the visual gap for a novel door opening task using imitation learning in a new visual domain. Visit the project website at https://retinagan.github.io/}},
  keywords = {}
}


@misc{dqn,
	title = {{Playing atari with deep reinforcement learning}},
	author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
	archivePrefix = "arXiv", 
    note = {arXiv:1312.5602},	
	year = {2013}
}

@article{rl_cyclegan,
  year     = {2020},
  title    = {{RL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real}},
  author   = {Rao, Kanishka and Harris, Chris and Irpan, Alex and Levine, Sergey and Ibarz, Julian and Khansari, Mohi},
  journal  = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi      = {10.1109/cvpr42600.2020.01117},
  abstract = {{Deep neural network based reinforcement learning (RL) can learn appropriate visual representations for complex tasks like vision-based robotic grasping without the need for manually engineering or prior learning a perception system. However, data for RL is collected via running an agent in the desired environment, and for applications like robotics, running a robot in the real world may be extremely costly and time consuming. Simulated training offers an appealing alternative, but ensuring that policies trained in simulation can transfer effectively into the real world requires additional machinery. Simulations may not match reality, and typically bridging the simulation-to-reality gap requires domain knowledge and task-specific engineering. We can automate this process by employing generative models to translate simulated images into realistic ones. However, this sort of translation is typically task-agnostic, in that the translated images may not preserve all features that are relevant to the task. In this paper, we introduce the RL-scene consistency loss for image translation, which ensures that the translation operation is invariant with respect to the Q-values associated with the image. This allows us to learn a task-aware translation. Incorporating this loss into unsupervised domain translation, we obtain RL-CycleGAN, a new approach for simulation-to-real-world transfer for re-inforcement learning. In evaluations of RL-CycleGAN on two vision-based robotics grasping tasks, we show that RL-CycleGAN offers a substantial improvement over a number of prior methods for sim-to-real transfer, attaining excellent real-world performance with only a modest number of realworld observations.}},
  pages    = {11154--11163},
  volume   = {00},
  note     = {RL-CycleGAN}
}

@article{enhancing_photorealism_enhancement,
  title   = {Enhancing Photorealism Enhancement},
  author  = {Stephan R. Richter and Hassan Abu AlHaija and Vladlen Koltun},
  journal = {arXiv:2105.04619},
  year    = {2021}
}



@article{playable_environments,
  year     = {2022},
  title    = {{Playable Environments: Video Manipulation in Space and Time}},
  author   = {Menapace, Willi and Lathuilière, Stéphane and Siarohin, Aliaksandr and Theobalt, Christian and Tulyakov, Sergey and Golyanik, Vladislav and Ricci, Elisa},
  journal  = {arXiv},
  eprint   = {2203.01914},
  abstract = {{We present Playable Environments - a new representation for interactive video generation and manipulation in space and time. With a single image at inference time, our novel framework allows the user to move objects in 3D while generating a video by providing a sequence of desired actions. The actions are learnt in an unsupervised manner. The camera can be controlled to get the desired viewpoint. Our method builds an environment state for each frame, which can be manipulated by our proposed action module and decoded back to the image space with volumetric rendering. To support diverse appearances of objects, we extend neural radiance fields with style-based modulation. Our method trains on a collection of various monocular videos requiring only the estimated camera parameters and 2D object locations. To set a challenging benchmark, we introduce two large scale video datasets with significant camera movements. As evidenced by our experiments, playable environments enable several creative applications not attainable by prior video synthesis works, including playable 3D video generation, stylization and manipulation. Further details, code and examples are available at https://willi-menapace.github.io/playable-environments-website}},
}


@article{dual_diffusion,
  year     = {2022},
  title    = {{Dual Diffusion Implicit Bridges for Image-to-Image Translation}},
  author   = {Su, Xuan and Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal  = {arXiv},
  eprint   = {2203.08382},
  abstract = {{Common image-to-image translation methods rely on joint training over data from both source and target domains. This excludes cases where domain data is private (e.g., in a federated setting), and often means that a new model has to be trained for a new pair of domains. We present Dual Diffusion Implicit Bridges (DDIBs), an image translation method based on diffusion models, that circumvents training on domain pairs. DDIBs allow translations between arbitrary pairs of source-target domains, given independently trained diffusion models on the respective domains. Image translation with DDIBs is a two-step process: DDIBs first obtain latent encodings for source images with the source diffusion model, and next decode such encodings using the target model to construct target images. Moreover, DDIBs enable cycle-consistency by default and is theoretically connected to optimal transport. Experimentally, we apply DDIBs on a variety of synthetic and high-resolution image datasets, demonstrating their utility in example-guided color transfer, image-to-image translation as well as their connections to optimal transport methods.}},
}


@article{lpips,
  year      = {2018},
  title     = {{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}},
  author    = {Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  journal   = {arXiv},
  eprint    = {1801.03924},
  abstract  = {{While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.}},
}


@article{ners, 
year = {2021}, 
  title = {{NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild}}, 
  author = {Zhang, Jason Y and Yang, Gengshan and Tulsiani, Shubham and Ramanan, Deva}, 
  journal = {arXiv}, 
  eprint = {2110.07604}, 
  abstract = {{Recent history has seen a tremendous growth of work exploring implicit representations of geometry and radiance, popularized through Neural Radiance Fields (NeRF). Such works are fundamentally based on a (implicit) volumetric representation of occupancy, allowing them to model diverse scene structure including translucent objects and atmospheric obscurants. But because the vast majority of real-world scenes are composed of well-defined surfaces, we introduce a surface analog of such implicit models called Neural Reflectance Surfaces (NeRS). NeRS learns a neural shape representation of a closed surface that is diffeomorphic to a sphere, guaranteeing water-tight reconstructions. Even more importantly, surface parameterizations allow NeRS to learn (neural) bidirectional surface reflectance functions (BRDFs) that factorize view-dependent appearance into environmental illumination, diffuse color (albedo), and specular "shininess." Finally, rather than illustrating our results on synthetic scenes or controlled in-the-lab capture, we assemble a novel dataset of multi-view images from online marketplaces for selling goods. Such "in-the-wild" multi-view image sets pose a number of challenges, including a small number of views with unknown/rough camera estimates. We demonstrate that surface-based neural reconstructions enable learning from such data, outperforming volumetric neural rendering-based reconstructions. We hope that NeRS serves as a first step toward building scalable, high-quality libraries of real-world shape, materials, and illumination. The project page with code and video visualizations can be found at https://jasonyzhang.com/ners.}}, 
}



@article{unit,
  year     = {2017},
  title    = {{Unsupervised Image-to-Image Translation Networks}},
  author   = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
  journal  = {arXiv},
  eprint   = {1703.00848},
  abstract = {{Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit .}},
  note     = {UNIT},
}


@article{nerf,
  year     = {2020},
  title    = {{NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis}},
  author   = {Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  journal  = {arXiv},
  eprint   = {2003.08934},
  abstract = {{We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslashtheta, \textbackslashphi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.}},
}

@article{deferred_neural_rendering,
  year     = {2019},
  title    = {{Deferred Neural Rendering: Image Synthesis using Neural Textures}},
  author   = {Thies, Justus and Zollhöfer, Michael and Nießner, Matthias},
  journal  = {arXiv},
  eprint   = {1904.12356},
  abstract = {{The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings. To address this challenging problem, we introduce Deferred Neural Rendering, a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components. Specifically, we propose Neural Textures, which are learned feature maps that are trained as part of the scene capture process. Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect. In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains. For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space. This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates. We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks.}},
}

@article{gancraft,
  year     = {2021},
  title    = {{GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds}},
  author   = {Hao, Zekun and Mallya, Arun and Belongie, Serge and Liu, Ming-Yu},
  journal  = {arXiv},
  eprint   = {2104.07659},
  abstract = {{We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/ .}},
}


@article{cyclegan,
  year      = {2017},
  title     = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
  author    = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  journal   = {arXiv},
  eprint    = {1703.10593},
  abstract  = {{Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X \textbackslashrightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y \textbackslashrightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) \textbackslashapprox X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.}},
}


@article{fast_nerf, 
year = {2021}, 
title = {{FastNeRF: High-Fidelity Neural Rendering at 200FPS}}, 
author = {Garbin, Stephan J and Kowalski, Marek and Johnson, Matthew and Shotton, Jamie and Valentin, Julien}, 
journal = {arXiv}, 
eprint = {2103.10380}, 
abstract = {{Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.}}, 
}


@article{stable_view_synthesis,
  year      = {2020},
  title     = {{Stable View Synthesis}},
  author    = {Riegler, Gernot and Koltun, Vladlen},
  journal   = {arXiv},
  eprint    = {2011.07233},
  abstract  = {{We present Stable View Synthesis (SVS). Given a set of source images depicting a scene from freely distributed viewpoints, SVS synthesizes new views of the scene. The method operates on a geometric scaffold computed via structure-from-motion and multi-view stereo. Each point on this 3D scaffold is associated with view rays and corresponding feature vectors that encode the appearance of this point in the input images. The core of SVS is view-dependent on-surface feature aggregation, in which directional feature vectors at each 3D point are processed to produce a new feature vector for a ray that maps this point into the new target view. The target view is then rendered by a convolutional network from a tensor of features synthesized in this way for all pixels. The method is composed of differentiable modules and is trained end-to-end. It supports spatially-varying view-dependent importance weighting and feature transformation of source images at each point; spatial and temporal stability due to the smooth dependence of on-surface feature aggregation on the target view; and synthesis of view-dependent effects such as specular reflection. Experimental results demonstrate that SVS outperforms state-of-the-art view synthesis methods both quantitatively and qualitatively on three diverse real-world datasets, achieving unprecedented levels of realism in free-viewpoint video of challenging large-scale scenes. Code is available at https://github.com/intel-isl/StableViewSynthesis}},
}


@article{nerv,
  year      = {2020},
  title     = {{NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis}},
  author    = {Srinivasan, Pratul P and Deng, Boyang and Zhang, Xiuming and Tancik, Matthew and Mildenhall, Ben and Barron, Jonathan T},
  journal   = {arXiv},
  eprint    = {2012.03927},
  abstract  = {{We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.}},
}


@article{free_view_synthesis,
  year      = {2020},
  title     = {{Free View Synthesis}},
  author    = {Riegler, Gernot and Koltun, Vladlen},
  journal   = {arXiv},
  eprint    = {2008.05511},
  abstract  = {{We present a method for novel view synthesis from input images that are freely distributed around a scene. Our method does not rely on a regular arrangement of input views, can synthesize images for free camera movement through the scene, and works for general scenes with unconstrained geometric layouts. We calibrate the input images via SfM and erect a coarse geometric scaffold via MVS. This scaffold is used to create a proxy depth map for a novel view of the scene. Based on this depth map, a recurrent encoder-decoder network processes reprojected features from nearby views and synthesizes the new view. Our network does not need to be optimized for a given scene. After training on a dataset, it works in previously unseen environments with no fine-tuning or per-scene optimization. We evaluate the presented approach on challenging real-world datasets, including Tanks and Temples, where we demonstrate successful view synthesis for the first time and substantially outperform prior and concurrent work.}},
}

@article{barf,
  author     = {Chen{-}Hsuan Lin and
                Wei{-}Chiu Ma and
                Antonio Torralba and
                Simon Lucey},
  title      = {{BARF:} Bundle-Adjusting Neural Radiance Fields},
  journal    = {CoRR},
  volume     = {abs/2104.06405},
  year       = {2021},
  url        = {https://arxiv.org/abs/2104.06405},
  eprinttype = {arXiv},
  eprint     = {2104.06405},
  timestamp  = {Mon, 19 Apr 2021 16:45:47 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2104-06405.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{mocyclegan,
  year      = {2019},
  title     = {{Mocycle-GAN}},
  author    = {Amsaleg, Laurent and Huet, Benoit and Larson, Martha and Gravier, Guillaume and Hung, Hayley and Ngo, Chong-Wah and Ooi, Wei Tsang and Chen, Yang and Pan, Yingwei and Yao, Ting and Tian, Xinmei and Mei, Tao},
  journal   = {Proceedings of the 27th ACM International Conference on Multimedia},
  doi       = {10.1145/3343031.3350937},
  eprint    = {1908.09514},
  abstract  = {{Unsupervised image-to-image translation is the task of translating an image from one domain to another in the absence of any paired training examples and tends to be more applicable to practical applications. Nevertheless, the extension of such synthesis from image-to-image to video-to-video is not trivial especially when capturing spatio-temporal structures in videos. The difficulty originates from the aspect that not only the visual appearance in each frame but also motion between consecutive frames should be realistic and consistent across transformation. This motivates us to explore both appearance structure and temporal continuity in video synthesis. In this paper, we present a new Motion-guided Cycle GAN, dubbed as Mocycle-GAN, that novelly integrates motion estimation into unpaired video translator. Technically, Mocycle-GAN capitalizes on three types of constrains: adversarial constraint discriminating between synthetic and real frame, cycle consistency encouraging an inverse translation on both frame and motion, and motion translation validating the transfer of motion between consecutive frames. Extensive experiments are conducted on video-to-labels and labels-to-video translation, and superior results are reported when comparing to state-of-the-art methods. More remarkably, we qualitatively demonstrate our Mocycle-GAN for both flower-to-flower and ambient condition transfer.}},
  pages     = {647--655},
  keywords  = {},
}

@article{cut,
  author    = {Taesung Park and
               Alexei A. Efros and
               Richard Zhang and
               Jun{-}Yan Zhu},
  title     = {Contrastive Learning for Unpaired Image-to-Image Translation},
  journal   = {CoRR},
  volume    = {abs/2007.15651},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.15651},
  eprinttype = {arXiv},
  eprint    = {2007.15651},
  timestamp = {Mon, 03 Aug 2020 14:32:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-15651.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{tecogan,
  year      = {2020},
  title     = {{Learning temporal coherence via self-supervision for GAN-based video generation}},
  author    = {Chu, Mengyu and Xie, You and Mayer, Jonas and Leal-Taixé, Laura and Thuerey, Nils},
  journal   = {ACM Transactions on Graphics (TOG)},
  issn      = {0730-0301},
  doi       = {10.1145/3386569.3392457},
  eprint    = {1811.09393},
  abstract  = {{Our work explores temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationships in the generated data are much less explored. Natural temporal changes are crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. Additionally, we propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirm the rankings computed with these metrics. Code, data, models, and results are provided at https://github.com/thunil/TecoGAN.}},
  pages     = {75:1--75:13},
  number    = {4},
  volume    = {39},
  keywords  = {},
  local-url = {file://localhost/Users/ryan/Downloads/tecogan_video_translation.pdf}
}

@article{nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}

@inproceedings{dnerf,
    title={{D-NeRF: Neural Radiance Fields for Dynamic Scenes}},
    author={Pumarola, Albert and Corona, Enric and Pons-Moll, Gerard and Moreno-Noguer, Francesc},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year={2020}
}

@inproceedings{nerualradianceflow,
  author    = {Yilun Du and Yinan Zhang and Hong-Xing Yu 
               and Joshua B. Tenenbaum and Jiajun Wu},
  title     = {Neural Radiance Flow for 4D View Synthesis and Video Processing},
  year      = {2021},
  booktitle   = CVPR,
}
                


@misc{gan,
  doi = {10.48550/ARXIV.1406.2661},
  
  url = {https://arxiv.org/abs/1406.2661},
  
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Generative Adversarial Networks},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{stylegan,
  doi = {10.48550/ARXIV.1812.04948},
  
  url = {https://arxiv.org/abs/1812.04948},
  
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{dalle,
  doi = {10.48550/ARXIV.2102.12092},
  
  url = {https://arxiv.org/abs/2102.12092},
  
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Zero-Shot Text-to-Image Generation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{imagen,
  doi = {10.48550/ARXIV.2205.11487},
  
  url = {https://arxiv.org/abs/2205.11487},
  
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{pixtopix,
  doi = {10.48550/ARXIV.1611.07004},
  
  url = {https://arxiv.org/abs/1611.07004},
  
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Image-to-Image Translation with Conditional Adversarial Networks},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
